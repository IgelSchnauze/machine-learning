Задание 6
- Описать класс алгоритма Decision Tree
1. Инициализация c 4 параметрами для ограничения обучения (макс длина дерева, мин энтропия в узле, максимум элементов в конечном узле, число интервалов для подразбиения значений характеристики для поиска лучшего сплита) +
2. Функции обучения дерева - простая и векторная
3. Функция predict +
4. Функция подсчета accuracy +
5. Функции сохранения/загрузки дерева в pickle файлы. +
Функции работы с выборками
6. Формирование обучающей/тестовой/валидационной выборок +
Алгоритм обучать на датасете digits из sklearn    +
- Описать класс ячейки дерева Decision Tree +
1. Инициализация с ссылками на левое и правое поддерево (2), характеристикой, по которой происходит сплит и её значением (2), переменной, хранящей значение является ли узел терминальным, и вектором, хранящим вектор вероятности принадлежности класса (2). Итого 6 параметров. +

Требования:
1) Разделяющая функция - разделение гиперплоскостью, число потомков = 2  +
2) Наивная реализация - 50% задания, векторная - ещё 50%, при выполнении всех остальных пунктов
3) Валидация по макс длине дерева (от 1 до 10), максимуму элементов в конечном узле (от 2 до 8 c шагом в два), мин энтропии в узле (от 0 до 0.3 с шагом в 0.1) (10*4*4 = всего 160 экспериментов). При наивной реализации это займет долгое время (~ около часа на старом core i5). Убедитесь, что код работает корректно, прежде чем ставить большой эксперимент. 
Сохраните лучшую модель в пикл со значением лучших параметров в названии и приложите к коду
4) Отдельно построить график с поведением точности decision tree на обучающей  и валидационной выборке при изменении максимальной длины дерева от 1 до 10 (максимум элементов в конечном узле=1, энтропии=0) +
5) Запрограммировать корректное вычисление энтропии и деление выборки без переполнения памяти +?
6) Confusion Matrix для лучшего RF на тестовой выборке +?
7) Число интервалов для подразбиения значений характеристики для поиска лучшего сплита везде считать равным 10 +
8) Точность на тестовой выборке не ниже 85%! -

